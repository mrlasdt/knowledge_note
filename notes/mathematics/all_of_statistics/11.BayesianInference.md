
# Bayesian Inference
* There are two type of statistical methods 
  * Frequentist: 
    * Probability refers to limiting relative frequencies. Probabilities are objective properties of the real world.
    * Parameters are fixed, unknown constants. 
    * Statistical procedures should be designed to have well-defined long run frequency properties.
  * Bayesian: 
    * Probability describes degree of belief, not limiting frequency. 
    * We can make probability statements about parameters, even though they are fixed constants.
    * We make inferences about a parameter $\theta$ by producing a probability distribution for $\theta$.
  
***the Bayesian method***
1. Chose a probability density $f(\theta)$ - called the **prior distribution** - that expresses our beliefs about a parameter $\theta$ before we see any data.
2. We choose a statistical model $f(x|\theta)$ that reflects our beliefs about x given $\theta$.
3. After observing data $X_1, \dots ,X_n$, we update our beliefs and caculate the posterior distribution $f(\theta| X_1, \dots, X_n)$.

Update methods: 
   * *Use Bayes theorem*
    Let $A_1, \dots , A_k$ be a partition of $\Omega$ such that $\mathbb{P}(A_i) > 0$  for each $i$. If $\mathbb{P}(B) > 0$, then for each $i = 1, \dots, k$
    $$ \mathbb{P} (A_{i} | B) = \frac{\mathbb{P}(B|A_{i}) \mathbb{P}(A_{i}) }{\sum_{j} \mathbb{P}(B| A_{i} )\mathbb{P}(A_{i})} $$ 

Get log of nominator:
$$\log(\mathbb{P}(B|A_i)\mathbb{P}(A_i)) = \log(\mathbb{P}(B|A_i) + \log(\mathbb{P}(A_i))$$
$\to$ the probability of observed data point $B$ added(updated the believe) to probability of events $A_i$

